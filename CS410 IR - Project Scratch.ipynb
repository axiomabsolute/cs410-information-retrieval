{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of IR System\n",
    "\n",
    "Components\n",
    "1. Interface\n",
    "2. Tokenizer\n",
    "3. Indexer\n",
    "4. Indexes\n",
    "5. Scorer\n",
    "6. Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from music21 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from collections import defaultdict, Counter\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def get_parts_for_piece(piece):\n",
    "    return piece.parts\n",
    "\n",
    "def get_parts(pieces):\n",
    "    for piece in pieces:\n",
    "        try:\n",
    "            scores = [piece.getScoreByNumber(number) for number in piece.getNumbers()]\n",
    "            for score in scores:\n",
    "                for part in score.parts:\n",
    "                    yield (score.metadata.title, part.partName, part)\n",
    "        except:\n",
    "            for part in piece.parts:\n",
    "                yield (piece.metadata.title, part.partName, part)\n",
    "\n",
    "def get_notes(part):\n",
    "    return flatten([m.notes for m in part.measures(0, None) if m.isStream])\n",
    "\n",
    "# When we get snippets, we need to account for chords. To handle this, treat each note value as a set and compute the cartesian\n",
    "# product to produce all possible one-line snippets for the part\n",
    "def get_snippets_for_piece(piece_name, part_name, notes, snippet_length):\n",
    "    return [Snippet(piece_name, part_name, notes[i: i+snippet_length], i) for i in range(0, 1 + len(notes) - snippet_length)]\n",
    "\n",
    "def get_snippets_for_parts(parts):\n",
    "    return flatten(list((get_snippets_for_piece(part[0], part[1], get_notes(part[2]), 5) for part in parts)))\n",
    "\n",
    "def get_snippets_for_pieces(pieces):\n",
    "    parts = list(get_parts(pieces))\n",
    "    return get_snippets_for_parts(parts)\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self, index_methods, scorers = None, pieces = []):\n",
    "        self.index_methods = index_methods\n",
    "        parts = list(get_parts(pieces))\n",
    "        self.piece_names = set( (part[0] for part in parts) )\n",
    "        # Break up pieces into a flat list of snippets\n",
    "        snippets = get_snippets_for_parts(parts)\n",
    "        # For each index_method, build an index\n",
    "        self.indexes = { k:Index(snippets, v, k) for k,v in index_methods.items() }\n",
    "        # Store scorers\n",
    "        self.scorers = scorers\n",
    "                        \n",
    "    def add_piece(self, piece):\n",
    "        parts = get_parts(pieces)\n",
    "        snippets = get_snippets_for_parts(parts)\n",
    "        for snippet in snippets:\n",
    "            for index in self.indexes.values():\n",
    "                index.add_snippet(snippet)\n",
    "    \n",
    "    def lookup(self, query):\n",
    "        snippets_by_index_type = {index_name: index.lookup(query) for index_name,index in self.indexes.items()}\n",
    "        print(snippets_by_index_type)\n",
    "        return {scorer_name: scorer(snippets_by_index_type) for scorer_name,scorer in self.scorers.items()}\n",
    "        # return {scorer_name: scorer(snippets_by_index_type) for scorer_name, scorer in self.scorers.items()}\n",
    "#         result = {}\n",
    "#         for index_name,index in self.indexes.items():\n",
    "#             local_result = {}\n",
    "#             snippets = index.lookup(query)\n",
    "#             for scorer_name,scorer in self.scorers.items():\n",
    "#                 local_result[scorer_name] = scorer(snippets)\n",
    "#             results[index_name] = local_result\n",
    "#         return result\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"IRSystem(%s pieces)\" % (len(self.piece_names))\n",
    "                        \n",
    "\n",
    "class Snippet:\n",
    "    def __init__(self, piece_name, part, notes, offset):\n",
    "        self.piece = piece_name\n",
    "        self.part = part\n",
    "        self.notes = notes\n",
    "        self.offset = offset\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Snippet(%s, %s, %s, %s)\" % (self.piece, self.part, self.offset, [note.pitch.nameWithOctave for note in self.notes])\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, snippets, keyfn, name = \"\"):\n",
    "        self.index = defaultdict(set)\n",
    "        self.keyfn = keyfn\n",
    "        self.name = name\n",
    "        for snippet in snippets:\n",
    "            self.add_snippet(snippet)\n",
    "            \n",
    "    def add_snippet(self, snippet):\n",
    "        for key in self.keyfn(snippet):\n",
    "            self.index[key].add(snippet)\n",
    "            \n",
    "    def lookup(self, query):\n",
    "        return flatten([list(self.index[key]) for key in self.keyfn(query)])\n",
    "    \n",
    "    def merge_indexes(self, index):\n",
    "        result = Index([], self.keyfn, self.name)\n",
    "        for k,v in self.index.items():\n",
    "            for item in v:\n",
    "                result.index[k].add(item)\n",
    "        for k,v in index.index.items():\n",
    "            for item in v:\n",
    "                result.index[k].add(item)\n",
    "        return result\n",
    "    \n",
    "    def __repr__(self):\n",
    "        result = \"Index[%s]\\n\" % self.name\n",
    "        for key, values in self.index.items():\n",
    "            result += \"  %s - %s\\n\" % (key, len(values))\n",
    "        return result\n",
    "        #return \"Index[%s]\\n  %s\" % (self.name, '\\n  '.join(self.index.keys()))\n",
    "    \n",
    "def merge_indexes(indexes):\n",
    "    return reduce(lambda x,y: x.merge_indexes(y), indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers\n",
    "def wrap_query_as_piece(query):\n",
    "    piece = stream.Score()\n",
    "    part = stream.Part()\n",
    "    part.partName = \"query\"\n",
    "    measures = converter.parse(\"tinynotation: %s\" % query)\n",
    "    for measure in measures:\n",
    "        part.append(measure)\n",
    "    piece.append(part)\n",
    "    return piece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indexers\n",
    "# An indexer is a function from snippet to a list of strings representing the index key\n",
    "def get_pitches(snippet):\n",
    "    return [note.pitch for note in snippet.notes]\n",
    "\n",
    "def index_key_by_pitch(snippet):\n",
    "    return [' '.join([pitch.nameWithOctave for pitch in get_pitches(snippet)])]\n",
    "\n",
    "# Parm looks like [ [index_1_piece_1, index_2_piece_2, ...], [index_1_piece_2, index_2_piece_2, ...]... ]\n",
    "def merge_indexes_for_pieces(indexes_by_piece):\n",
    "    collected_indexes = zip(*indexes_by_piece)\n",
    "    return [ merge_indexes(index_collection) for index_collection in collected_indexes ]\n",
    "\n",
    "def make_indexes(index_methods, snippets_by_part):\n",
    "    print(snippets_by_part)\n",
    "    return [ [ Index(snippets, index_method, name) for name, index_method in index_methods.items() ] for snippets in snippets_by_part ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scorer\n",
    "# A scorer is a function: Function[Dictionary[index_type, snippets], Dictionary[piece_name, number]\n",
    "\n",
    "# Each time the query matches a snippet from a piece for each index, give it a score of 1, and sum them\n",
    "def simple_sum_scorer(snippets_by_index_type):\n",
    "    snippets = flatten(snippets_by_index_type.values())\n",
    "    return Counter([snippet.piece for snippet in snippets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "shared_part_success = 'C4 D E8 F G16 A B c'\n",
    "shared_part_failure = \"c4 c'4 c''4 CC4 C4\"\n",
    "time_signature = '4/4'\n",
    "tokenizer_test_query = '%s %s' % (time_signature, shared_part_success)\n",
    "test_piece = \"%s %s %s CC4 C4 c4\" % (time_signature, shared_part_failure, shared_part_success)\n",
    "test_piece_fail = \"%s %s C4 c4\" % (time_signature, shared_part_failure)\n",
    "\n",
    "pieces = list((corpus.parse(piece) for piece in corpus.getComposer('bach')[:5]))\n",
    "\n",
    "index_methods = {\n",
    "    'By Pitch': index_key_by_pitch\n",
    "}\n",
    "\n",
    "scorer_methods = {\n",
    "    'Simple Sum': simple_sum_scorer\n",
    "}\n",
    "\n",
    "def tokenizer_test(query, test_piece):\n",
    "    query_piece = wrap_query_as_piece(query)\n",
    "    test_piece = wrap_query_as_piece(test_piece)\n",
    "    return (query_piece, test_piece)\n",
    "\n",
    "def snippets_test(piece_name, piece, snippet_length = 5):\n",
    "    parts = get_parts_for_piece(piece)\n",
    "    snippets_by_part = [ get_snippets_for_piece(piece_name, part.partName, get_notes(part), snippet_length) for part in parts ]\n",
    "    return snippets_by_part\n",
    "\n",
    "#query_piece, test_piece = tokenizer_test(tokenizer_test_query, test_piece)\n",
    "#test_piece_fail = wrap_query_as_piece(test_piece_fail)\n",
    "#snippets_by_part = snippets_test(\"Test Piece\", test_piece)\n",
    "#fail_snippets_by_part = snippets_test(\"Fail Test Piece\", test_piece_fail)\n",
    "#merge_indexes_for_pieces(make_indexes(index_methods, flatten([snippets_by_part, fail_snippets_by_part])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "irsystem = IRSystem(index_methods, scorer_methods, pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work in iPython\n",
    "# pieces[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'By Pitch': [Snippet(bwv1.6.mxl, Tenor, 45, ['F4', 'E4', 'D4', 'E4', 'C4']), Snippet(bwv1.6.mxl, Tenor, 14, ['F4', 'E4', 'D4', 'E4', 'C4'])]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Simple Sum': Counter({'bwv1.6.mxl': 2})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_snippets = get_snippets_for_pieces(pieces)\n",
    "random_snippet = random.choice(all_snippets)\n",
    "irsystem.lookup(random_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
